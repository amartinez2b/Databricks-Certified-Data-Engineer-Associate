{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd2f28a-7598-4c96-bb3d-b1f52915164f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dabc72e6-e691-4b95-a05c-03716c2d6f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Crear sesi√≥n Spark\n",
    "spark = SparkSession.builder.appName(\"FakerComplexData\").getOrCreate()\n",
    "\n",
    "fake = Faker(\"es_ES\")\n",
    "\n",
    "data = []\n",
    "for i in range(10):\n",
    "    direccion = {\n",
    "        \"calle\": fake.street_name(),\n",
    "        \"ciudad\": fake.city(),\n",
    "        \"pais\": fake.country()\n",
    "    }\n",
    "\n",
    "    # üîÅ Generar n√∫meros base\n",
    "    base_phones = [fake.phone_number() for _ in range(random.randint(1, 2))]\n",
    "\n",
    "    # üîÅ Repetir algunos n√∫meros y agregar nulos\n",
    "    telefonos = []\n",
    "    for _ in range(random.randint(1, 5)):\n",
    "        choice = random.choice([\"repeat\", \"null\", \"new\"])\n",
    "        if choice == \"repeat\":\n",
    "            telefonos.append(random.choice(base_phones))\n",
    "        elif choice == \"null\":\n",
    "            telefonos.append(None)\n",
    "        else:\n",
    "            telefonos.append(fake.phone_number())\n",
    "\n",
    "    # üõí Compras\n",
    "    compras = [\n",
    "        {\"producto\": fake.word(), \"precio\": round(random.uniform(10, 500), 2)}\n",
    "        for _ in range(random.randint(1, 4))\n",
    "    ]\n",
    "\n",
    "    data.append({\n",
    "        \"id\": i + 1,\n",
    "        \"nombre\": fake.name(),\n",
    "        \"edad\": random.randint(18, 70),\n",
    "        \"direccion\": direccion,\n",
    "        \"telefonos\": telefonos,\n",
    "        \"compras\": compras\n",
    "    })\n",
    "\n",
    "# üß± Esquema del DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"edad\", IntegerType(), True),\n",
    "    StructField(\"direccion\", StructType([\n",
    "        StructField(\"calle\", StringType(), True),\n",
    "        StructField(\"ciudad\", StringType(), True),\n",
    "        StructField(\"pais\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"telefonos\", ArrayType(StringType())),\n",
    "    StructField(\"compras\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"producto\", StringType(), True),\n",
    "            StructField(\"precio\", DoubleType(), True)\n",
    "        ])\n",
    "    ))\n",
    "])\n",
    "\n",
    "# üöÄ Crear el DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.createOrReplaceTempView(\"customer_shop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8cff314-84d3-462d-9c22-395750bf97ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "old_orders = [\n",
    "    {\"_id\":1,\"product\":\"Televisor\",\"quantity\":1,\"price\":345.32},\n",
    "    {\"_id\":2,\"product\":\"Refrigerador\",\"quantity\":1,\"price\":533.01},\n",
    "    {\"_id\":3,\"product\":\"Sofa\",\"quantity\":2,\"price\":200.78},\n",
    "    {\"_id\":4,\"product\":\"Silla\",\"quantity\":4,\"price\":20.44},\n",
    "    {\"_id\":5,\"product\":\"Mesa\",\"quantity\":1,\"price\":120.99}\n",
    "]\n",
    "spark.sql(\"DROP TABLE IF EXISTS old_orders\")\n",
    "spark.createDataFrame(old_orders).write.mode(\"overwrite\").saveAsTable(\"old_orders\")\n",
    "new_orders = [\n",
    "    {\"_id\":1,\"product\":\"Televisor\",\"quantity\":1,\"price\":345.32},\n",
    "    {\"_id\":2,\"product\":\"Refrigerador\",\"quantity\":1,\"price\":533.01},\n",
    "    {\"_id\":3,\"product\":\"Sofa\",\"quantity\":2,\"price\":200.78},\n",
    "    {\"_id\":4,\"product\":\"Silla de playa\",\"quantity\":3,\"price\":35.44},\n",
    "    {\"_id\":5,\"product\":\"Sombrilla\",\"quantity\":1,\"price\":10.99}\n",
    "]\n",
    "spark.sql(\"DROP TABLE IF EXISTS new_orders\")\n",
    "spark.createDataFrame(new_orders).write.mode(\"overwrite\").saveAsTable(\"new_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d7fb82-826c-4418-b88c-f6b62ae4cef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from faker import Faker\n",
    "import random\n",
    "fake = Faker(\"es_ES\")\n",
    "productos = [\"Laptop\", \"Smartphone\", \"Auriculares\", \"Teclado\", \"Monitor\"]\n",
    "dias_semana = [\"1_LUN\", \"2_MAR\", \"3_MIE\", \"4_JUE\", \"5_VIE\", \"6_SAB\", \"7_DOM\"]\n",
    "data = []\n",
    "for _ in range(100):\n",
    "    producto = random.choice(productos)\n",
    "    dia = random.choice(dias_semana)\n",
    "    valor_venta = round(random.uniform(50, 1500), 2)\n",
    "    data.append((dia, producto, valor_venta))\n",
    "\n",
    "# === Definir esquema ===\n",
    "schema = StructType([\n",
    "    StructField(\"dia_semana\", StringType(), True),\n",
    "    StructField(\"producto\", StringType(), True),\n",
    "    StructField(\"valor_venta\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# === Crear DataFrame ===\n",
    "df_ventas = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# === Registrar vista temporal (opcional) ===\n",
    "spark.sql(\"DROP TABLE IF EXISTS ventas\")\n",
    "df_ventas.write.mode(\"overwrite\").saveAsTable(\"ventas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774a6eee-d495-4326-ae1c-49c1583863f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker(\"es_ES\")\n",
    "\n",
    "# Generar datos falsos\n",
    "data = []\n",
    "for _ in range(10):\n",
    "    region = fake.city()\n",
    "    producto = random.choice([\"Laptop\", \"Smartphone\", \"Monitor\", \"Auriculares\", \"Teclado\"])\n",
    "    ventas_enero = round(random.uniform(1000, 5000), 2)\n",
    "    ventas_febrero = round(random.uniform(1000, 5000), 2)\n",
    "    ventas_marzo = round(random.uniform(1000, 5000), 2)\n",
    "    data.append((region, producto, ventas_enero, ventas_febrero, ventas_marzo))\n",
    "\n",
    "# Definir esquema\n",
    "schema = StructType([\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"producto\", StringType(), True),\n",
    "    StructField(\"ventas_enero\", DoubleType(), True),\n",
    "    StructField(\"ventas_febrero\", DoubleType(), True),\n",
    "    StructField(\"ventas_marzo\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Crear DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"sales_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a514fd16-d32f-4184-b7cc-2069fdf77940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Inicializar Faker\n",
    "fake = Faker(\"es_ES\")\n",
    "dominios = [\"gmail.com\", \"outlook.com\", \"yahoo.com\", \"hotmail.com\", \"icloud.com\", \"company.com\", \"fundacion.org\", \"universidad.edu\", \"banco.fin\"]\n",
    "# === Generar datos fake ===\n",
    "data = []\n",
    "for i in range(1, 21):\n",
    "    nombre = fake.first_name()\n",
    "    apellido = fake.last_name()\n",
    "\n",
    "    # Crear correo con dominio real\n",
    "    dominio = random.choice(dominios)\n",
    "    correo = f\"{nombre.lower()}.{apellido.lower()}@{dominio}\"\n",
    "\n",
    "    # Direcci√≥n como struct\n",
    "    direccion = {\n",
    "        \"calle\": fake.street_address(),\n",
    "        \"ciudad\": fake.city(),\n",
    "        \"pais\": fake.country()\n",
    "    }\n",
    "\n",
    "    data.append({\n",
    "        \"customer_id\": str(i),\n",
    "        \"nombre\": nombre,\n",
    "        \"apellido\": apellido,\n",
    "        \"email\": correo,\n",
    "        \"telefono\": fake.phone_number(),\n",
    "        \"empresa\": fake.company(),\n",
    "        \"cargo\": random.choice([\"Gerente\", \"Analista\", \"Director\", \"Vendedor\", \"Asistente\"]),\n",
    "        \"direccion\": direccion,\n",
    "        \"fecha_registro\": str(fake.date_between(start_date='-2y', end_date='today'))\n",
    "    })\n",
    "\n",
    "# === Definir esquema ===\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"apellido\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"telefono\", StringType(), True),\n",
    "    StructField(\"empresa\", StringType(), True),\n",
    "    StructField(\"cargo\", StringType(), True),\n",
    "    StructField(\"direccion\", StructType([\n",
    "        StructField(\"calle\", StringType(), True),\n",
    "        StructField(\"ciudad\", StringType(), True),\n",
    "        StructField(\"pais\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"fecha_registro\", StringType(), True)\n",
    "])\n",
    "\n",
    "# === Crear DataFrame ===\n",
    "customers = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Registrar vista temporal para SQL\n",
    "spark.sql(\"DROP TABLE IF EXISTS customers\")\n",
    "customers.write.mode(\"overwrite\").saveAsTable(\"customers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66efbab5-1fe2-443e-92ca-45ee691a0424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, DateType\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "fake = Faker(\"es_ES\")\n",
    "\n",
    "# =======================\n",
    "# 1Ô∏è‚É£ DATAFRAME: CUSTOMERS\n",
    "# =======================\n",
    "customers_data = []\n",
    "for i in range(1, 11):  # 10 clientes\n",
    "    customers_data.append({\n",
    "        \"customer_id\": str(i),\n",
    "        \"email\": f\"{fake.first_name().lower()}.{fake.last_name().lower()}@{random.choice(['gmail.com','yahoo.com','outlook.com'])}\",\n",
    "        \"profile\": random.choice([\"Regular\", \"Premium\", \"Gold\"]),\n",
    "        \"updated\": str(fake.date_between(start_date='-1y', end_date='today'))\n",
    "    })\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"profile\", StringType(), True),\n",
    "    StructField(\"updated\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "\n",
    "# =======================\n",
    "# 2Ô∏è‚É£ DATAFRAME: BOOKS\n",
    "# =======================\n",
    "categorias = [\"Ficci√≥n\", \"Ciencia\", \"Historia\", \"Romance\", \"Fantas√≠a\", \"Autoayuda\"]\n",
    "books_data = []\n",
    "for i in range(1, 16):  # 15 libros\n",
    "    books_data.append({\n",
    "        \"book_id\": str(i),\n",
    "        \"title\": fake.sentence(nb_words=3).replace(\".\", \"\"),\n",
    "        \"author\": f\"{fake.first_name()} {fake.last_name()}\",\n",
    "        \"category\": random.choice(categorias),\n",
    "        \"price\": round(random.uniform(10, 100), 2)\n",
    "    })\n",
    "\n",
    "books_schema = StructType([\n",
    "    StructField(\"book_id\", StringType(), False),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "books = spark.createDataFrame(books_data, schema=books_schema)\n",
    "\n",
    "# =======================\n",
    "# 3Ô∏è‚É£ DATAFRAME: ORDERS\n",
    "# =======================\n",
    "orders_data = []\n",
    "for i in range(1, 21):  # 20 pedidos\n",
    "    customer = random.choice(customers_data)\n",
    "    n_books = random.randint(1, 3)\n",
    "    selected_books = random.sample(books_data, n_books)\n",
    "\n",
    "    order_books = [b[\"book_id\"] for b in selected_books]\n",
    "    total = sum(b[\"price\"] for b in selected_books)\n",
    "    quantity = n_books\n",
    "\n",
    "    orders_data.append({\n",
    "        \"order_id\": str(i),\n",
    "        \"order_date\": str(fake.date_between(start_date='-6M', end_date='today')),\n",
    "        \"customer_id\": customer[\"customer_id\"],\n",
    "        \"quantity\": quantity,\n",
    "        \"total\": round(total, 2),\n",
    "        \"books\": order_books\n",
    "    })\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"total\", DoubleType(), True),\n",
    "    StructField(\"books\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "orders = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "\n",
    "# Crear vistas temporales\n",
    "customers.write.mode(\"overwrite\").saveAsTable(\"customer\")\n",
    "books.write.mode(\"overwrite\").saveAsTable(\"book\")\n",
    "orders.write.mode(\"overwrite\").saveAsTable(\"order\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GenData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
