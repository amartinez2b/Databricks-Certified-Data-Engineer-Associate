{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f19fdf-d79c-4bbf-8061-c5d699a45ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"volume_path\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a308dfba-6849-433d-a0ff-ed991dab8f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume_path = dbutils.widgets.get(\"volume_path\")\n",
    "if volume_path == \"\":\n",
    "  raise Exception(\"Please provide a volume path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b205d19b-625f-4f1e-ab21-e46a5d2ab0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, DateType\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2f8d8b-2df8-41cf-a8e7-ead56c74f100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_new_data():\n",
    "    fake = Faker(\"es_ES\")\n",
    "    # =======================\n",
    "    # 1️⃣ DATAFRAME: CUSTOMERS\n",
    "    # =======================\n",
    "    customers_data = []\n",
    "    for i in range(1, 11):  # 10 clientes\n",
    "        customers_data.append({\n",
    "            \"customer_id\": str(i),\n",
    "            \"email\": f\"{fake.first_name().lower()}.{fake.last_name().lower()}@{random.choice(['gmail.com','yahoo.com','outlook.com'])}\",\n",
    "            \"profile\": random.choice([\"Regular\", \"Premium\", \"Gold\"]),\n",
    "            \"updated\": str(fake.date_between(start_date='-1y', end_date='today'))\n",
    "        })\n",
    "\n",
    "    customers_schema = StructType([\n",
    "        StructField(\"customer_id\", StringType(), False),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"profile\", StringType(), True),\n",
    "        StructField(\"updated\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    customers = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "\n",
    "    # =======================\n",
    "    # 2️⃣ DATAFRAME: BOOKS\n",
    "    # =======================\n",
    "    categorias = [\"Ficción\", \"Ciencia\", \"Historia\", \"Romance\", \"Fantasía\", \"Autoayuda\"]\n",
    "    books_data = []\n",
    "    for i in range(1, 16):  # 15 libros\n",
    "        books_data.append({\n",
    "            \"book_id\": str(i),\n",
    "            \"title\": fake.sentence(nb_words=3).replace(\".\", \"\"),\n",
    "            \"author\": f\"{fake.first_name()} {fake.last_name()}\",\n",
    "            \"category\": random.choice(categorias),\n",
    "            \"price\": round(random.uniform(10, 100), 2)\n",
    "        })\n",
    "\n",
    "    books_schema = StructType([\n",
    "        StructField(\"book_id\", StringType(), False),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"price\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    books = spark.createDataFrame(books_data, schema=books_schema)\n",
    "\n",
    "    # =======================\n",
    "    # 3️⃣ DATAFRAME: ORDERS\n",
    "    # =======================\n",
    "    orders_data = []\n",
    "    number_of_orders = random.randint(1,5)\n",
    "    for i in range(1, number_of_orders): \n",
    "        customer = random.choice(customers_data)\n",
    "        n_books = random.randint(1, 3)\n",
    "        selected_books = random.sample(books_data, n_books)\n",
    "\n",
    "        order_books = [b[\"book_id\"] for b in selected_books]\n",
    "        total = sum(b[\"price\"] for b in selected_books)\n",
    "        quantity = n_books\n",
    "\n",
    "        orders_data.append({\n",
    "            \"order_id\": str(uuid.uuid4()),\n",
    "            \"order_date\": str(fake.date_between(start_date='-6M', end_date='today')),\n",
    "            \"customer_id\": customer[\"customer_id\"],\n",
    "            \"quantity\": quantity,\n",
    "            \"total\": round(total, 2),\n",
    "            \"books\": order_books\n",
    "        })\n",
    "\n",
    "    orders_schema = StructType([\n",
    "        StructField(\"order_id\", StringType(), False),\n",
    "        StructField(\"order_date\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"total\", DoubleType(), True),\n",
    "        StructField(\"books\", ArrayType(StringType()), True)\n",
    "    ])\n",
    "\n",
    "    orders = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "    orders.repartition(1).write.mode(\"append\").format(\"parquet\").save(f\"{volume_path}/orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b4dc804-abe7-4b0f-8f77-8957dadd3e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_new_data()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/agustin.martinez@bigdataybi.com/Databricks-Certified-Data-Engineer-Associate/requirements.txt"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GenDataStream",
   "widgets": {
    "volume_path": {
     "currentValue": "",
     "nuid": "ba9d97c8-6f18-49e1-9f6b-e566ecc44872",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "volume_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "volume_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
