
# Databricks Asset Bundle

# ðŸ“¦ Databricks Asset Bundles

## 1. Â¿QuÃ© son los Asset Bundles?

Un **Databricks Asset Bundle** es una forma de **empaquetar y desplegar recursos de Databricks** (notebooks, trabajos, dashboards, pipelines, etc.) como una unidad lÃ³gica.  
EstÃ¡n pensados para **facilitar el desarrollo, versionamiento y despliegue** de proyectos en Databricks, especialmente cuando se integran con herramientas de **DevOps/CI/CD**.

ðŸ‘‰ En otras palabras: un *bundle* es un **paquete declarativo** que describe quÃ© recursos quieres desplegar en un workspace y cÃ³mo deben comportarse.

---

## 2. Â¿Para quÃ© sirven?

Los Asset Bundles ayudan a:

- ðŸš€ **Automatizar despliegues:** evitar configuraciones manuales en el workspace.  
- ðŸ‘¥ **ColaboraciÃ³n en equipo:** todos usan el mismo paquete con la misma definiciÃ³n.  
- ðŸ”„ **Versionamiento:** almacenar la definiciÃ³n del bundle en Git y controlar cambios.  
- ðŸ› ï¸ **Multi-entorno:** permitir despliegues consistentes en distintos entornos (dev, test, prod).  
- âš¡ **Infraestructura como cÃ³digo (IaC):** describir notebooks, clusters, jobs y pipelines en archivos YAML.

---

## 3. Componentes de un Bundle

Un bundle se define con archivos de configuraciÃ³n, tÃ­picamente en **YAML**.  
Ejemplo bÃ¡sico de un bundle:

```yaml
bundle:
  name: mi-proyecto

resources:
  jobs:
    my-job:
      name: "ETL Job"
      tasks:
        - task_key: etl_task
          notebook_task:
            notebook_path: ./notebooks/etl_notebook
          existing_cluster_id: "1234-567890-cluster"
```

### ExplicaciÃ³n:
- bundle: nombre del paquete.
- resources: lista de recursos incluidos (jobs, notebooks, clusters, dashboards).
- jobs: define un job que se ejecuta en Databricks.
- notebook_path: especifica la ruta al notebook.

## 4. Beneficios principales
1. Consistencia: mismo cÃ³digo, misma configuraciÃ³n en todos los entornos.
2. Productividad: ahorra tiempo evitando despliegues manuales.
3. Escalabilidad: se pueden gestionar muchos recursos de forma centralizada.
4. IntegraciÃ³n DevOps: funcionan muy bien con pipelines de CI/CD (GitHub Actions, Azure DevOps, Jenkins, etc.).


## Proyecto

### Requisitos

Tener instalado el cliente de databricks

### Crear proyecto

Vamos a crear un nuevo proyecto mediante un pequeÃ±o formulario

```sh
cd databricks-project
databricks bundle init -p [profile]
Search: â–ˆ
? Template to use: 
>  default-python (The default Python template for Notebooks and Lakeflow)
  default-sql
  dbt-sql
  mlops-stacks
â†“ experimental-jobs-as-code
Unique name for this project [my_project]:
Include a stub (sample) notebook in 'my_project/src': 
  â–¸ yes
    no
Include a stub (sample) Lakeflow Declarative Pipeline in 'my_project/src': 
  â–¸ yes
    no
Include a stub (sample) Python package in 'my_project/src': 
  â–¸ yes
    no
Use serverless compute: 
  â–¸ yes
    no
```

Dentro del proyecto vamos a construir un flujo en desarrollo (dev)

En el archivo my_project/databricks.yml agrega las variables my_catalog y my_schema

```yml
# This is a Databricks asset bundle definition for my_project.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.
# Especifica el nombre del proyecto
bundle:
  name: my_project
  uuid: 40b11951-439c-4619-8bb7-db8a627192f2

artifacts:
  python_artifact:
    type: whl
    build: uv build --wheel
# Especifica los recursos a incluir en el bundle como jobs y pipelines
include:
  - resources/*.yml
  - resources/*/*.yml

# >> NUEVO*** Crea variables para reutilizar en el job
variables:
  my_catalog:
    description: Catalogo por defecto
    default: workspace
  my_schema:
    description: Esquema por defecto
    default: bookstore_dev
# Especifica los entornos de despliegue
targets:
  dev:
    # The default target uses 'mode: development' to create a development copy.
    # - Deployed resources get prefixed with '[dev my_user_name]'
    # - Any job schedules and triggers are paused by default.
    # See also https://docs.databricks.com/dev-tools/bundles/deployment-modes.html.
    mode: development
    default: true
    workspace:
      host: https://dbc-f23c1e0b-86c8.cloud.databricks.com

  prod:
    mode: production
    workspace:
      host: https://dbc-f23c1e0b-86c8.cloud.databricks.com
      # We explicitly deploy to /Workspace/Users/agustin.martinez@bigdataybi.com to make sure we only have a single copy.
      root_path: /Workspace/Users/agustin.martinez@bigdataybi.com/.bundle/${bundle.name}/${bundle.target}
    permissions:
      - user_name: agustin.martinez@bigdataybi.com
        level: CAN_MANAGE

```

Ahora modifica el notebook my_project/notebook.ipynb agrega este parrafo:

```python

dbutils.widgets.text("catalog", "")
dbutils.widgets.text("schema", "")
catalog = dbutils.widgets.get("catalog")
schema = dbutils.widgets.get("schema")
print(f"Catalog: {catalog}, Schema: {schema}")

```

Luego envia las variables como parametros en el job my_project/resources/my_project.job.yml

```yml
# The main job for my_project.
resources:
  jobs:
    my_project_job:
      name: my_project_job

      trigger:
        # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
        periodic:
          interval: 1
          unit: DAYS

      #email_notifications:
      #  on_failure:
      #    - your_email@example.com

      tasks:
        - task_key: notebook_task
          notebook_task:
            notebook_path: ../src/notebook.ipynb
            base_parameters:
              catalog: ${var.my_catalog}
              schema: ${var.my_schema}     

        - task_key: refresh_pipeline
          depends_on:
            - task_key: notebook_task
          pipeline_task:
            pipeline_id: ${resources.pipelines.my_project_pipeline.id}

        - task_key: main_task
          depends_on:
            - task_key: refresh_pipeline
          environment_key: default
          python_wheel_task:
            package_name: my_project
            entry_point: main

      # A list of task execution environment specifications that can be referenced by tasks of this job.
      environments:
        - environment_key: default

          # Full documentation of this spec can be found at:
          # https://docs.databricks.com/api/workspace/jobs/create#environments-spec
          spec:
            environment_version: "2"
            dependencies:
              - ../dist/*.whl

```

### Despliega el bundle

```sh
cd my_project
databricks bundle deploy --target dev
databricks bundle deploy --target prod
```

### Ejecuta el pipeline 

```sh
# Lista los jobs
databricks jobs list -p [profile]
databricks jobs run-now  [job_id] -p [profile] 
```