{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a02c03a-7a4a-437e-9a21-c343ee68c07e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow\n",
    "\n",
    "## ¿Qué es Data Lakeflow y para qué sirve?\n",
    "\n",
    "Data LakeFlow es una herramienta diseñada para orquestar, automatizar y gestionar flujos de datos dentro de un Data Lake. Permite definir, programar y monitorear procesos de ingestión, transformación y movimiento de datos de manera eficiente y escalable. Su objetivo principal es facilitar la integración de datos desde múltiples fuentes, asegurar la calidad y trazabilidad de los datos, y optimizar el procesamiento para análisis avanzado y toma de decisiones empresariales.\n",
    "\n",
    "## Conceptos básicos\n",
    "\n",
    "### Tablas Streaming\n",
    "- Procesamiento incremental\n",
    "- Exactamente una vez\n",
    "- Usadas para la ingesta de datos\n",
    "- Cargas solo de tipo append (una sola inserción)\n",
    "- Transmisión de baja latencia\n",
    "\n",
    "Ejemplo de uso\n",
    "\n",
    "```sql\n",
    "CREATE STREAMING TABLE web_clicks\n",
    "AS \n",
    "SELECT * FROM STREAM read_files('/volume/path');\n",
    "```\n",
    "-----\n",
    "```sql\n",
    "CREATE STREAMING TABLE server_logs\n",
    "AS \n",
    "SELECT from_json(...) data\n",
    "FROM STREAM read_kafka(...)\n",
    "```\n",
    "\n",
    "### Vistas Materializadas\n",
    "- Resultados de consultas almacenadas en caché con actualizaciones programadas\n",
    "- Precalculo para un acceso rápido\n",
    "- Puede ser incremental pero no necesariamente\n",
    "- Usado para transformaciones complejas\n",
    "- Consultas analíticas\n",
    "- Resultados siempre correctos\n",
    "- Acelera las consultas en los dashboards\n",
    "- Mejora la frescura de los datos\n",
    "\n",
    "Ejemplo de uso\n",
    "\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW customers_orders\n",
    "AS \n",
    "SELECT \n",
    " customers.name,\n",
    " sum(orders.amount),\n",
    " orders.orderdate\n",
    "FROM orders\n",
    "LEFT JOIN customers ON \n",
    "  orders.custkey = customers.c_custkey\n",
    "GROUP BY \n",
    "name,\n",
    "orderdate;\n",
    "```\n",
    "\n",
    "### Flujos\n",
    "- Alimenta a tablas streaming y a vistas materializadas\n",
    "- Admite semántica streaming o batch\n",
    "\n",
    "\n",
    "## Data Quality\n",
    "\n",
    "La calidad de los datos se basa en expectativas (expectations) los cuales:\n",
    "\n",
    "- Definen la calidad de los datos y los controles de integridad dentro de la canalización con expectativas\n",
    "- Aborda los errores de calidad con políticas flexibles como fail, drop, alert, quarantine\n",
    "- Todas las ejecuciones de pipelines y métricas de calidad se capturan, rastrean e informan\n",
    "\n",
    "Ejemplo de uso:\n",
    "\n",
    "```sql\n",
    "CREATE STREAMING TABLE fire_account_bronze \n",
    "AS \n",
    "( \n",
    "  CONSTRAINT valid_account_open_dt EXPECT (account_dt. is note null and (account_close_dt > account_open_dt)) ON VIOLATION DROP ROW\n",
    "  COMMENT \"Bronze table with valid account ids\"\n",
    "  SELECT * FROM fire_account_raw ...\n",
    ")\n",
    "```\n",
    "\n",
    "## Development vs Production\n",
    "\n",
    "Integración rápida \n",
    "\n",
    "### Development mode\n",
    "- Reutiliza un clúster para una iteración rápida\n",
    "- Sin re-intentos en caso de error, lo que permite una interación más rápida\n",
    "- Para ejecuciones activadas desde el editor o la interfaz de creación\n",
    "\n",
    "### Production mode\n",
    "\n",
    "- Reduce los costos apagando los clústers en cuanto terminan (menos de 5 minutos)\n",
    "- Usa re-intentos escalonados inclidos los reinicios de clústers y garantizan la fiabilidad ante problemas transitorios\n",
    "- Detiene los pipelines programados\n",
    "\n",
    "## Modulariza tu código\n",
    "Evida codificar de forma rígida las rutas, nombres, catálogos o esquemas\n",
    "\n",
    "La configuración de un pipeline es un mala de **key-value pais** que puede utilizarse para parametrizar el código. Esto permite:\n",
    "\n",
    "- Mejorar la legibilidad del codigo\n",
    "- Reutilizar el codigo en varias cadenas\n",
    "\n",
    "Ejemplo de uso:\n",
    "\n",
    "- Ve a la opción de configuración del pipeline y crea una variable \"volumen\" con una ruta asignada /path\n",
    "- En el código puedes leer la variable asi:\n",
    "\n",
    "SQL:\n",
    "\n",
    "```sql\n",
    "CREATE STREAMING TABLE  data AS\n",
    "SELECT * FROM read_files(\"${volumen}\",\"json\")\n",
    "```\n",
    "\n",
    "Python:\n",
    "\n",
    "```python\n",
    "@dp.table\n",
    "def data():\n",
    "  input_path = spark.conf.get(\"volumen\")\n",
    "  spark.readStream.format(\"cloudFiles\").load(input_path)\n",
    "``` \n",
    "\n",
    "## Observabilidad\n",
    "\n",
    "Contiene toda la información relacionada con el pipeline, tablas o control de datos\n",
    "\n",
    "```sql\n",
    "select *\n",
    "from event_log(table( catalog.schema.table_name ));\n",
    "\n",
    "select * from event_log('pipeline-id');\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.2 - DataLakeFlow",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
