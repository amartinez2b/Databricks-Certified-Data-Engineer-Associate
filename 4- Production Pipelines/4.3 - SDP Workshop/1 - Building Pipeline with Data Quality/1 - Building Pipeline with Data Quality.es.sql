-- Databricks notebook source
-- MAGIC %md
-- MAGIC # Lecci√≥n 1: Creaci√≥n de Pipelines de Datos con Calidad de Datos
-- MAGIC
-- MAGIC ## Objetivos de aprendizaje
-- MAGIC Al finalizar esta lecci√≥n, podr√°s:
-- MAGIC - Crear un Lakeflow Spark Declarative Pipeline usando el editor multi‚Äëarchivo
-- MAGIC - Implementar arquitectura medallion (Bronze ‚Üí Silver ‚Üí Gold)
-- MAGIC - Aplicar expectativas de calidad de datos con manejo correcto de violaciones
-- MAGIC - Procesar datos incrementalmente con Auto Loader
-- MAGIC - Monitorear la ejecuci√≥n del pipeline y ver resultados
-- MAGIC
-- MAGIC ## Duraci√≥n: ~50 minutos
-- MAGIC
-- MAGIC ## Prerrequisitos
-- MAGIC - Haber ejecutado el cuaderno **0-SETUP.py**

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## ¬øQu√© es Lakeflow Spark Declarative Pipelines?
-- MAGIC
-- MAGIC **Lakeflow Spark Declarative Pipelines** (anteriormente Delta Live Tables) es un marco declarativo para construir pipelines de datos confiables, mantenibles y comprobables.
-- MAGIC
-- MAGIC ### Funcionalidades clave:
-- MAGIC
-- MAGIC - **Declarativo**: Define el qu√©, no el c√≥mo
-- MAGIC - **Gesti√≥n autom√°tica de dependencias**: El sistema determina el orden de ejecuci√≥n
-- MAGIC - **Calidad de datos integrada**: Las expectativas hacen cumplir la calidad en la ingesta
-- MAGIC - **Procesamiento incremental**: Solo procesa autom√°ticamente los datos nuevos
-- MAGIC - **Organizaci√≥n multi‚Äëarchivo**: Organiza el c√≥digo l√≥gicamente en varios archivos
-- MAGIC - **IDE integrado**: Entorno completo con DAG, vistas previas y monitoreo
-- MAGIC
-- MAGIC ### El Editor de Lakeflow Pipelines:
-- MAGIC
-- MAGIC A diferencia de los notebooks tradicionales, el nuevo **Lakeflow Pipelines Editor** ofrece:
-- MAGIC - Edici√≥n multi‚Äëarchivo con pesta√±as
-- MAGIC - Gr√°fico del pipeline (DAG) en vivo
-- MAGIC - Vistas previas de datos en l√≠nea
-- MAGIC - Monitoreo de rendimiento integrado
-- MAGIC - Ejecuci√≥n selectiva (ejecutar archivo, tabla o pipeline)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## A. Comprender la arquitectura del pipeline
-- MAGIC
-- MAGIC En esta lecci√≥n construiremos un pipeline sencillo siguiendo la **arquitectura medallion**:
-- MAGIC
-- MAGIC ```
-- MAGIC ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
-- MAGIC ‚îÇ   BRONZE    ‚îÇ     ‚îÇ   SILVER    ‚îÇ     ‚îÇ    GOLD     ‚îÇ
-- MAGIC ‚îÇ             ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ             ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ             ‚îÇ
-- MAGIC ‚îÇ   Raw Data  ‚îÇ     ‚îÇ Clean Data  ‚îÇ     ‚îÇ Aggregated  ‚îÇ
-- MAGIC ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
-- MAGIC ```
-- MAGIC
-- MAGIC ### Nuestro pipeline:
-- MAGIC
-- MAGIC 1. **Bronze**: `bronze.orders`
-- MAGIC    - Ingesta archivos JSON sin procesar desde el almacenamiento
-- MAGIC    - Conserva todos los datos fuente
-- MAGIC    - Agrega metadatos (hora de procesamiento, archivo de origen)
-- MAGIC
-- MAGIC 2. **Silver**: `silver.orders_clean`
-- MAGIC    - Analiza y valida tipos de datos
-- MAGIC    - Aplica expectativas de calidad de datos
-- MAGIC    - Selecciona columnas relevantes
-- MAGIC
-- MAGIC 3. **Gold**: `gold.order_summary`
-- MAGIC    - Agregaciones de negocio
-- MAGIC    - Res√∫menes diarios de pedidos
-- MAGIC    - Listo para anal√≠tica/reportes

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## B. Crea tu primer pipeline
-- MAGIC
-- MAGIC Vamos a crear un pipeline usando el Lakeflow Pipelines Editor.
-- MAGIC
-- MAGIC ### Paso 1: Abrir el creador de pipelines
-- MAGIC
-- MAGIC 1. En la **barra lateral izquierda**, haz clic en **New** (bot√≥n azul arriba)
-- MAGIC 2. Selecciona **ETL pipeline**
-- MAGIC
-- MAGIC ### Paso 2: Configurar ajustes b√°sicos
-- MAGIC
-- MAGIC 1. **Nombre del pipeline**: `SDP Workshop - [tu-usuario]`
-- MAGIC    - Ejemplo: `SDP Workshop - john_doe`
-- MAGIC
-- MAGIC 2. **Cat√°logo predeterminado**: Tu cat√°logo
-- MAGIC    - Fue creado por el cuaderno de setup
-- MAGIC    - Formato `sdp_workshop_john_doe`
-- MAGIC
-- MAGIC 3. **Esquema predeterminado**: `bronze`
-- MAGIC
-- MAGIC 4. **Opci√≥n de creaci√≥n**: Selecciona **"Add existing assets"**
-- MAGIC    - Pipeline root folder: selecciona la carpeta que importaste **Build Data Pipelines with Lakeflow Spark Declarative Pipeline**
-- MAGIC    - Source code paths: selecciona **transformations**
-- MAGIC
-- MAGIC 5. Haz clic en **Add**
-- MAGIC
-- MAGIC Ahora deber√≠as ver el **Lakeflow Pipelines Editor** con:
-- MAGIC - Explorador de assets del pipeline a la izquierda
-- MAGIC - Editor de c√≥digo en el centro
-- MAGIC - Gr√°fico del pipeline vac√≠o a la derecha

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## C. Explora la interfaz del editor de pipelines
-- MAGIC
-- MAGIC Antes de agregar c√≥digo, entendamos la interfaz:
-- MAGIC
-- MAGIC ### Panel izquierdo: Explorador de assets
-- MAGIC
-- MAGIC - **Pesta√±a Pipeline**: Muestra archivos del proyecto
-- MAGIC - **Pesta√±a All files**: Acceso a todo tu workspace
-- MAGIC - **Icono Settings** (‚öôÔ∏è): Configuraci√≥n del pipeline
-- MAGIC - **Icono Schedule** (üìÖ): Programar ejecuciones
-- MAGIC - **Icono Share** (üë•): Gestionar permisos
-- MAGIC
-- MAGIC ### Panel central: Editor de c√≥digo
-- MAGIC
-- MAGIC - Interfaz con pesta√±as para m√∫ltiples archivos
-- MAGIC - Resaltado de sintaxis para SQL y Python
-- MAGIC - Controles de ejecuci√≥n en la barra de herramientas
-- MAGIC
-- MAGIC ### Panel derecho: Gr√°fico del pipeline (DAG)
-- MAGIC
-- MAGIC - Representaci√≥n visual del flujo de datos
-- MAGIC - Se actualiza en tiempo real durante la ejecuci√≥n
-- MAGIC - Haz clic en nodos para ver vista previa de datos
-- MAGIC
-- MAGIC ### Panel inferior: Detalles y monitoreo
-- MAGIC
-- MAGIC - **Tables**: Lista de tablas con m√©tricas
-- MAGIC - **Performance**: Perfiles de ejecuci√≥n
-- MAGIC - **Issues**: Errores y advertencias
-- MAGIC - **Event Log**: Eventos de ejecuci√≥n detallados

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## D. Configurar el pipeline
-- MAGIC
-- MAGIC Antes de agregar c√≥digo, configuremos el pipeline:
-- MAGIC
-- MAGIC ### Paso 1: Abrir Settings
-- MAGIC
-- MAGIC 1. Haz clic en el icono **Settings** (‚öôÔ∏è) en la barra lateral izquierda
-- MAGIC 2. El panel de configuraci√≥n se abre a la derecha
-- MAGIC
-- MAGIC ### Paso 2: Revisar ajustes generales
-- MAGIC
-- MAGIC - **Nombre del pipeline**: Debe coincidir con lo ingresado
-- MAGIC - **Modo del pipeline**: **Triggered** (predeterminado)
-- MAGIC - **Run as**: Tu usuario (no cambia sin permisos)
-- MAGIC
-- MAGIC ### Paso 3: Verificar assets de c√≥digo
-- MAGIC
-- MAGIC - **Root folder**: Carpeta del proyecto del pipeline
-- MAGIC - **Source code**: Debe estar vac√≠o o mostrar el archivo por defecto
-- MAGIC
-- MAGIC ### Paso 4: Confirmar ubicaci√≥n predeterminada
-- MAGIC
-- MAGIC - **Default catalog**: Tu cat√°logo
-- MAGIC - **Default schema**: `bronze`
-- MAGIC - Determinan d√≥nde se crean tablas si no se especifica
-- MAGIC
-- MAGIC ### Paso 5: Configurar c√≥mputo
-- MAGIC
-- MAGIC 1. Ve a la secci√≥n **Compute**
-- MAGIC 2. Aseg√∫rate que **Serverless** est√© seleccionado (recomendado)
-- MAGIC 3. Si no est√° disponible, classic compute funciona pero tarda m√°s en iniciar
-- MAGIC
-- MAGIC ### Paso 6: Agregar variable de configuraci√≥n ‚ö†Ô∏è IMPORTANTE
-- MAGIC
-- MAGIC Esto es cr√≠tico: el SQL referencia la variable `${source}`:
-- MAGIC
-- MAGIC 1. En **Configuration**, haz clic en **Add configuration**
-- MAGIC 2. **Key**: `source`
-- MAGIC 3. **Value**: Tu ruta de volumen del setup
-- MAGIC    - Formato: `/Volumes/{tu-catalogo}/default/raw`
-- MAGIC    - Ejemplo: `/Volumes/sdp_workshop_john_doe/default/raw`
-- MAGIC    - Si no recuerdas: Revisa la salida de 0-SETUP
-- MAGIC 4. Haz clic en **Save**
-- MAGIC
-- MAGIC ### Paso 7: Guardar ajustes
-- MAGIC
-- MAGIC - Cierra el panel de configuraci√≥n haciendo clic en el editor

-- COMMAND ----------

-- MAGIC %md
-- MAGIC
-- MAGIC ## E. Entender el c√≥digo del pipeline de pedidos
-- MAGIC
-- MAGIC Ve a la pesta√±a pipeline > transformations y abre `orders_pipeline.sql`
-- MAGIC
-- MAGIC Desglosemos lo que hace el c√≥digo:
-- MAGIC
-- MAGIC ### Capa Bronze - Ingesta Raw
-- MAGIC
-- MAGIC ```sql
-- MAGIC CREATE OR REFRESH STREAMING TABLE bronze.orders
-- MAGIC   COMMENT "Datos de pedidos sin procesar ingeridos desde archivos JSON"
-- MAGIC   TBLPROPERTIES ("pipelines.reset.allowed" = false)
-- MAGIC AS 
-- MAGIC SELECT *, current_timestamp() AS processing_time, ...
-- MAGIC FROM STREAM read_files("${source}/orders", format => 'json');
-- MAGIC ```
-- MAGIC
-- MAGIC **Conceptos clave**:
-- MAGIC - `CREATE OR REFRESH STREAMING TABLE`: Define una tabla actualizada incrementalmente
-- MAGIC - `STREAM read_files()`: Auto Loader - procesa archivos nuevos incrementalmente
-- MAGIC - `${source}`: Sustituci√≥n de variable desde la configuraci√≥n
-- MAGIC - `pipelines.reset.allowed = false`: Previene refrescos completos accidentales
-- MAGIC - El checkpoint se gestiona autom√°ticamente
-- MAGIC - Hereda cat√°logo y esquema por defecto o escribe con nombre totalmente calificado
-- MAGIC
-- MAGIC ### Capa Silver - Datos limpios
-- MAGIC
-- MAGIC ```sql
-- MAGIC CREATE OR REFRESH STREAMING TABLE silver.orders_clean
-- MAGIC   (
-- MAGIC     CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL) ON VIOLATION FAIL UPDATE,
-- MAGIC     CONSTRAINT valid_timestamp EXPECT (order_timestamp > "2020-01-01")
-- MAGIC   )
-- MAGIC AS SELECT ... FROM STREAM bronze.orders;
-- MAGIC ```
-- MAGIC
-- MAGIC **Conceptos clave**:
-- MAGIC - **Expectations**: Reglas de calidad de datos
-- MAGIC - `ON VIOLATION FAIL UPDATE`: Detiene el pipeline si falla
-- MAGIC - `FROM STREAM`: Lee incrementalmente desde bronze
-- MAGIC
-- MAGIC ### Capa Gold - L√≥gica de negocio
-- MAGIC
-- MAGIC ```sql
-- MAGIC CREATE OR REFRESH MATERIALIZED VIEW gold.orders_summary
-- MAGIC AS SELECT date(order_timestamp) AS order_date, count(*) AS total_daily_orders
-- MAGIC FROM silver.orders_clean
-- MAGIC GROUP BY date(order_timestamp);
-- MAGIC ```
-- MAGIC
-- MAGIC **Conceptos clave**:
-- MAGIC - `MATERIALIZED VIEW`: Resultados persistidos
-- MAGIC - Sin `STREAM`: Lee todos los datos de la fuente
-- MAGIC - Optimiza refresco incremental cuando es posible
-- MAGIC - Ideal para agregaciones y anal√≠tica

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## F. Validar el pipeline (Dry Run)
-- MAGIC
-- MAGIC Antes de ejecutar el pipeline, valid√©moslo:
-- MAGIC
-- MAGIC ### Paso 1: Ejecutar un Dry Run
-- MAGIC
-- MAGIC 1. En la **barra de herramientas** superior, haz clic en **Dry run**
-- MAGIC 2. Espera 1‚Äë2 minutos para completar la validaci√≥n
-- MAGIC 3. Observa el panel izquierdo ‚Äì mostrar√° modo "DRY RUN"
-- MAGIC
-- MAGIC ### Paso 2: Revisar resultados
-- MAGIC
-- MAGIC Tras completar:
-- MAGIC
-- MAGIC 1. **Pipeline Graph** (panel derecho):
-- MAGIC    - Debe mostrar 3 nodos: `orders` ‚Üí `orders_clean` ‚Üí `orders_summary`
-- MAGIC    - Flechas muestran el flujo
-- MAGIC    - No deben aparecer errores
-- MAGIC
-- MAGIC 2. **Tables** (panel inferior):
-- MAGIC    - Lista las 3 tablas
-- MAGIC    - Muestra cat√°logo, esquema y tipo
-- MAGIC    - Estado "Validated"
-- MAGIC
-- MAGIC 3. **Issues** (panel inferior):
-- MAGIC    - Debe estar vac√≠o
-- MAGIC    - Si hay errores, revisa tu configuraci√≥n
-- MAGIC
-- MAGIC ### Soluci√≥n de problemas (Dry Run):
-- MAGIC
-- MAGIC **Error: "Variable 'source' not found"**
-- MAGIC - Vuelve a Settings ‚Üí Configuration
-- MAGIC - Verifica que `source` est√© configurado correctamente
-- MAGIC
-- MAGIC **Error: "Schema not found"**
-- MAGIC - Aseg√∫rate de que 0-SETUP.py se ejecut√≥ correctamente
-- MAGIC - Verifica Default Catalog en Settings
-- MAGIC
-- MAGIC **Error: "Permission denied"**
-- MAGIC - Verifica privilegios CREATE en el cat√°logo
-- MAGIC - Contacta al admin si es necesario

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## G. Ejecuta una tabla a la vez
-- MAGIC
-- MAGIC El nuevo IDE facilita construir y probar iterativamente. En lugar de ejecutar todo el pipeline, puedes ejecutar solo una tabla.
-- MAGIC
-- MAGIC 1. Haz clic en `dataset actions`(‚ñ∂Ô∏è) arriba de `CREATE OR REFRESH STREAMING TABLE bronze.orders`
-- MAGIC 2. Selecciona **Run table** `sdp_workshop_{your_name}.bronze.orders`
-- MAGIC 2. En el Pipeline Graph ver√°s solo la tabla de pedidos ejecutada
-- MAGIC
-- MAGIC Tras completar, deber√≠as ver:
-- MAGIC
-- MAGIC - **orders** (bronze): 174 filas
-- MAGIC   - Se proces√≥ un archivo JSON (00.json)
-- MAGIC   - Todos los datos raw preservados

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## H. Ejecutar el pipeline
-- MAGIC
-- MAGIC ¬°Ahora ejecutemos el pipeline completo!
-- MAGIC
-- MAGIC ### Paso 1: Iniciar el pipeline
-- MAGIC
-- MAGIC 1. En la **barra de herramientas**, haz clic en **Run pipeline**
-- MAGIC 2. El pipeline comenzar√° a ejecutarse
-- MAGIC 3. Ver√°s el estado cambiar en el panel izquierdo
-- MAGIC
-- MAGIC ### Paso 2: Observar la ejecuci√≥n (2‚Äë3 minutos)
-- MAGIC
-- MAGIC Observa el **Pipeline Graph** mientras corre:
-- MAGIC
-- MAGIC 1. **Starting**: Nodos grises/azules
-- MAGIC 2. **Running**: Nodos amarillos con spinner
-- MAGIC 3. **Complete**: Nodos verdes con ‚úî
-- MAGIC 4. **Row counts**: N√∫meros en los bordes
-- MAGIC
-- MAGIC ### Paso 3: Resultados esperados (primer run)
-- MAGIC
-- MAGIC - **orders_clean** (silver): 174 filas
-- MAGIC   - Mismo conteo (validaci√≥n pas√≥)
-- MAGIC   - Se cumplieron los constraints
-- MAGIC
-- MAGIC - **order_summary** (gold): ~30 filas
-- MAGIC   - Agregado por fecha
-- MAGIC   - ~30 fechas √∫nicas en los datos

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## I. Explora los resultados del pipeline
-- MAGIC
-- MAGIC Veamos qu√© cre√≥ el pipeline:
-- MAGIC
-- MAGIC ### Paso 1: Calidad de datos
-- MAGIC
-- MAGIC 1. Clic en **orders_clean**
-- MAGIC 2. Busca la secci√≥n **Expectations**
-- MAGIC 3. Deber√≠as ver:
-- MAGIC    - **valid_order_id**: 174 cumplidas (0 violaciones)
-- MAGIC    - **valid_timestamp**: 174 cumplidas (0 violaciones)
-- MAGIC    - **valid_customer_id**: 174 cumplidas (0 violaciones)
-- MAGIC
-- MAGIC ### Paso 2: Ver datos agregados
-- MAGIC
-- MAGIC 1. Clic en **order_summary**
-- MAGIC 2. En la pesta√±a **Data** ver√°s:
-- MAGIC    - `order_date`: La fecha
-- MAGIC    - `total_daily_orders`: Conteo por fecha
-- MAGIC    - `unique_customers`: Clientes √∫nicos por fecha
-- MAGIC
-- MAGIC ### Paso 3: Revisar rendimiento
-- MAGIC
-- MAGIC 1. Clic en **Performance** en el panel inferior
-- MAGIC 2. Revisa m√©tricas:
-- MAGIC    - Duraci√≥n total: ~2‚Äë3 minutos (primer run)
-- MAGIC    - Tiempo por tabla
-- MAGIC    - Uso de recursos
-- MAGIC 3. Clic en cualquier tabla para ver profile

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## J. Procesamiento incremental
-- MAGIC
-- MAGIC Uno de los principales beneficios es el **procesamiento incremental autom√°tico**.
-- MAGIC
-- MAGIC ### Ejecutar de nuevo
-- MAGIC
-- MAGIC 1. Haz clic en **Run pipeline** nuevamente
-- MAGIC 2. Espera (ser√° r√°pido)
-- MAGIC 3. Observa:
-- MAGIC    - **0 filas nuevas** en todas las tablas
-- MAGIC    - Ejecuci√≥n m√°s r√°pida
-- MAGIC
-- MAGIC **¬øPor qu√©?** Auto Loader rastrea qu√© archivos ya fueron procesados.
-- MAGIC
-- MAGIC ### Agregar datos nuevos
-- MAGIC
-- MAGIC Agreguemos un archivo nuevo y veamos el procesamiento incremental:

-- COMMAND ----------

-- MAGIC %py
-- MAGIC import sys, os, re
-- MAGIC
-- MAGIC # Determinar la ra√≠z del pipeline autom√°ticamente (un nivel arriba)
-- MAGIC pipeline_root = os.path.dirname(os.getcwd())
-- MAGIC sys.path.append(pipeline_root)
-- MAGIC
-- MAGIC # Importar helper
-- MAGIC from utilities.utils import add_orders_file   # o utilities.utils si ese es tu archivo
-- MAGIC
-- MAGIC # Info de usuario actual
-- MAGIC current_user = spark.sql("SELECT current_user()").collect()[0][0]
-- MAGIC username = current_user.split("@")[0]
-- MAGIC
-- MAGIC # Limpiar username para nombres (remover caracteres especiales)
-- MAGIC clean_username = re.sub(r'[^a-z0-9]', '_', username.lower())
-- MAGIC
-- MAGIC working_dir = f'/Volumes/sdp_workshop_{clean_username}/default/raw'
-- MAGIC
-- MAGIC result = add_orders_file(spark, working_dir, file_number=1, num_orders=25)
-- MAGIC print(result)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### Ejecutar pipeline con datos nuevos
-- MAGIC
-- MAGIC 1. Vuelve a la pesta√±a del **pipeline editor**
-- MAGIC 2. Haz clic en **Run pipeline** de nuevo
-- MAGIC 3. Observa el gr√°fico
-- MAGIC 4. Observa:
-- MAGIC    - **+25 filas** en bronze y silver
-- MAGIC    - La capa gold se recomputa eficientemente
-- MAGIC    - Solo se ley√≥ el archivo NUEVO (01.json)
-- MAGIC
-- MAGIC **¬°Esto es procesamiento incremental!** El pipeline:
-- MAGIC - Detecta nuevos archivos autom√°ticamente
-- MAGIC - Procesa solo datos nuevos
-- MAGIC - Actualiza tablas downstream
-- MAGIC - Sin intervenci√≥n manual

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## K. Consulta las tablas de tu pipeline
-- MAGIC
-- MAGIC Las tablas creadas est√°n en Unity Catalog. ¬°Puedes consultarlas como cualquier otra!

-- COMMAND ----------

-- DBTITLE 1,Establecer Schema y Catalog
-- MAGIC %py
-- MAGIC # Establecer cat√°logo para consultas SQL en este notebook
-- MAGIC import re
-- MAGIC current_user = spark.sql("SELECT current_user()").collect()[0][0]
-- MAGIC username = current_user.split("@")[0]
-- MAGIC clean_username = re.sub(r'[^a-z0-9]', '_', username.lower())
-- MAGIC catalog_name = f"sdp_workshop_{clean_username}"
-- MAGIC
-- MAGIC # Usar como cat√°logo por defecto
-- MAGIC spark.sql(f"USE CATALOG {catalog_name}")
-- MAGIC print(f"‚úì Usando cat√°logo: {catalog_name}")
-- MAGIC print("  Todas las consultas SQL usar√°n este cat√°logo autom√°ticamente")

-- COMMAND ----------

-- Consultar la tabla bronze
SELECT * FROM bronze.orders LIMIT 10;

-- COMMAND ----------

-- Consultar la tabla silver
SELECT order_id, order_timestamp, customer_id 
FROM silver.orders_clean 
ORDER BY order_timestamp DESC
LIMIT 10;

-- COMMAND ----------

-- Consultar la agregaci√≥n gold
SELECT * FROM gold.order_summary 
ORDER BY order_date;

-- COMMAND ----------

-- Verificar conteos totales
SELECT 
  'orders' AS table_name, COUNT(*) AS row_count FROM bronze.orders
UNION ALL
SELECT 
  'orders_clean' AS table_name, COUNT(*) AS row_count FROM silver.orders_clean
UNION ALL
SELECT 
  'order_summary' AS table_name, COUNT(*) AS row_count FROM gold.order_summary;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## L. Puntos clave - Lecci√≥n 1
-- MAGIC
-- MAGIC ‚úÖ **Lakeflow Spark Declarative Pipelines** provee un marco declarativo para ETL  
-- MAGIC ‚úÖ **Editor multi‚Äëarchivo** ofrece experiencia tipo IDE con monitoreo integrado  
-- MAGIC ‚úÖ **Streaming tables** manejan procesamiento incremental autom√°ticamente  
-- MAGIC ‚úÖ **Materialized views** proveen agregaciones eficientes  
-- MAGIC ‚úÖ **Expectativas de calidad** hacen cumplir est√°ndares en la ingesta  
-- MAGIC ‚úÖ **Auto Loader** (`read_files`) simplifica la ingesta con checkpoints  
-- MAGIC ‚úÖ **Arquitectura medallion** organiza datos en Bronze ‚Üí Silver ‚Üí Gold  
-- MAGIC
-- MAGIC ## ¬øQu√© sigue?
-- MAGIC
-- MAGIC En la **Lecci√≥n 2**, ampliaremos estos conceptos para crear pipelines de producci√≥n con:
-- MAGIC - M√∫ltiples archivos de c√≥digo
-- MAGIC - Dependencias entre archivos
-- MAGIC - Uniones entre tablas en streaming
-- MAGIC - Programaci√≥n y monitoreo
-- MAGIC - Change Data Capture (CDC)
-- MAGIC
